{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain and large documents\n",
    "\n",
    "In this notebook we will apply the map-reduce chaining of langchain to summarize and discuss large documents.\n",
    "We will be analyzing scientific papers, as they follow a specific format, making it relatively easy to process.\n",
    "\n",
    "First, import all necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain langchain-community html2text tiktoken langchain-openai pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import dotenv\n",
    "import time\n",
    "\n",
    "dotenv.load_dotenv(\".env\", override=True) \n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "MODEL =\"gpt-4o\"\n",
    "chunk = 10000 # amount of data send to LLM per mapping "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_transformers import Html2TextTransformer\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import ReduceDocumentsChain, MapReduceDocumentsChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Read the PDF\n",
    "\n",
    "PDFs are built from printing, not so much for data processing. Extracting text from a PDF can be challenging. The same holds true from HTML pages.\n",
    "Some PDFs will therefore parse nicely, while others might create a mess. Given your set of documents, you might need to try out several PDF-readers and parsers to find the one that gives the best results. In this case we will be using PyPDFLoader.\n",
    "\n",
    "After reading the PDF we need to split it up in chunks. Picking the optimal chuck_size is tricky and depends on context length and LLM used. Setting it too low, however, will limit the reasoning capabilities of the LLM, because not enough context will then be provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loader = PyPDFLoader(\"crop_rotation_sugar_beet.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "html2text = Html2TextTransformer()\n",
    "docs = html2text.transform_documents(docs)\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=chunk, chunk_overlap=0\n",
    ")\n",
    "split_docs = text_splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The LLM used for processing the data\n",
    "\n",
    "In our case we will be using OpenAI GTP-4o.\n",
    "And we need a template for the mapping phase and a template for the reduce phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.3, model_name=MODEL, streaming=True)\n",
    "map_template = \"\"\"The following is a set of documents which combined form a full scientific paper and should therefore be considered as one long, single paper.\n",
    "{docs}\n",
    "{question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "reduce_template = \"\"\"{description}\n",
    "{doc_summaries}\n",
    "{question}\n",
    "Helpful Answer:\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The map-reduce chaining\n",
    "\n",
    "This is the most complex part: we need run the map-reduce approach on our document.\n",
    "Note: we will add a sleep() to the method to prevent too many calls to the API. It is possible to have a subscription to the OpenAI API with much higher limits, though.\n",
    "\n",
    "Documentation: [MapReduceDocumentsChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.map_reduce.MapReduceDocumentsChain.html#langchain.chains.combine_documents.map_reduce.MapReduceDocumentsChain)\n",
    "\n",
    "The map-reduce methods are reported as 'deprecated'. However, there is no a high level chain yet available. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runMapReduce(map_template, reduce_template, docs, llm, model = \"gpt-4o\"):\n",
    "    map_prompt = PromptTemplate.from_template(map_template)\n",
    "    map_chain = LLMChain(llm=llm, prompt=map_prompt )\n",
    "\n",
    "    # Reduce\n",
    "    reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "    reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
    "\n",
    "    # Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "    combine_documents_chain = StuffDocumentsChain(\n",
    "        llm_chain=reduce_chain, document_variable_name=\"doc_summaries\"\n",
    "    )\n",
    "\n",
    "    #print(\"Reduce phase\")\n",
    "    # Combines and iteravely reduces the mapped documents\n",
    "    reduce_documents_chain = ReduceDocumentsChain(\n",
    "        # This is final chain that is called.\n",
    "        combine_documents_chain=combine_documents_chain,\n",
    "        # If documents exceed context for `StuffDocumentsChain`\n",
    "        collapse_documents_chain=combine_documents_chain,\n",
    "        # The maximum number of tokens to group documents into.\n",
    "        #token_max=tokens,\n",
    "    )\n",
    "\n",
    "    #print(\"Mapping phase\")\n",
    "    # Combining documents by mapping a chain over them, then combining results\n",
    "    map_reduce_chain = MapReduceDocumentsChain(\n",
    "        # Map chain\n",
    "        llm_chain=map_chain,\n",
    "        # Reduce chain\n",
    "        reduce_documents_chain=reduce_documents_chain,\n",
    "        # The variable name in the llm_chain to put the documents in\n",
    "        document_variable_name=\"docs\",\n",
    "        # Return the results of the map steps in the output\n",
    "        return_intermediate_steps=False,\n",
    "    )\n",
    "\n",
    "    if model == \"gpt-4o\": # let's wait for a while\n",
    "        time.sleep(10)\n",
    "\n",
    "    return(map_reduce_chain.run(docs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a variable to keep track of all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryDoc = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic information on the paper\n",
    "\n",
    "During the mapping phase, we will go through the entire paper, looking for author names, name of the journal, etc. And we would like to know a little bit more on the journal itself.\n",
    "After collecting all this information, we will reduce this into a nice list, formatted in markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_q = \"Please identify the publisher of this scientific paper and the authors of this paper. Provide some background on the journal. Is it for example considered high impact? What are generally the topics and results shared in this journal? If you can not extract this information from this part of the text, just provide an empty string as answer.\"\n",
    "reduce_qDescription = \"The following contains an author list and information on the journal from a scientific paper:\"\n",
    "reduce_q = \"Take these and provide only the author list and information on the first mentioned journal and publisher. The output needs to be in Markdown file format.\"\n",
    "result = runMapReduce(map_template.format(question=map_q, docs=\"{docs}\"), reduce_template.format(description=reduce_qDescription, question = reduce_q, doc_summaries=\"{doc_summaries}\"), split_docs, llm=llm, model=MODEL)\n",
    "print(result)\n",
    "summaryDoc += result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research themes\n",
    "\n",
    "What is the paper really about?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_q = \"Please identify the main themes of this scientific paper.\"\n",
    "reduce_qDescription = \"The following is set of summaries from a scientific paper:\"\n",
    "reduce_q = \"Take these and distill it into a final, consolidated summary of the main themes of this scientific paper in Markdown file format.\"\n",
    "result = runMapReduce(map_template.format(question=map_q, docs=\"{docs}\"), reduce_template.format(description=reduce_qDescription, question = reduce_q, doc_summaries=\"{doc_summaries}\"), split_docs, llm=llm, model=MODEL)\n",
    "print(result)\n",
    "summaryDoc += result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summaries\n",
    "\n",
    "Create a summary of each of the sections of a scientific paper (might take some time to complete) and discuss the quality of each of these sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_q = \"Could you provide a summary of the introduction? If the document contains different sections of the paper you can answer with an empty string.\"\n",
    "reduce_qDescription = \"The following is set of summaries of the introduction of a scientific paper:\"\n",
    "reduce_q = \"Take these and distill it into a final, consolidated summary of the introduction section of this scientific paper in Markdown file format. Is the introduction complete and concise? Are there for example topics introduced which require more elaborations? \"\n",
    "result = runMapReduce(map_template.format(question=map_q, docs=\"{docs}\"), reduce_template.format(description=reduce_qDescription, question = reduce_q, doc_summaries=\"{doc_summaries}\"), split_docs, llm=llm, model=MODEL)\n",
    "#print(result)\n",
    "summaryDoc += result\n",
    "\n",
    "\n",
    "map_q = \"Could you provide a summary of the results section? You might need to deduce the section contains the results if the header is absent of labeled differently. If the document contains different sections of the paper you can answer with an empty string.\"\n",
    "reduce_qDescription = \"The following is set of summaries of the results section of a scientific paper:\"\n",
    "reduce_q = \"Take these and distill it into a final, consolidated summary of the results section of this scientific paper in Markdown file format. Is the results section concise, are there any conflicting results or are there any comments that should in either the conclusion or materials and methods section, for example?\"\n",
    "result = runMapReduce(map_template.format(question=map_q, docs=\"{docs}\"), reduce_template.format(description=reduce_qDescription, question = reduce_q, doc_summaries=\"{doc_summaries}\"), split_docs, llm=llm, model=MODEL)\n",
    "#print(result)\n",
    "summaryDoc += result\n",
    "\n",
    "map_q = \"Could you provide a summary of the Materials and Method section? Or any other section that might reflect the same content, such as an 'Implementation',  'Methods', 'Experimental Procedures' or 'Methodology' section. If the document contains different sections of the paper you can answer with an empty string.\"\n",
    "reduce_qDescription = \"The following is set of summaries of the Materials and Method section of a scientific paper:\"\n",
    "reduce_q = \"Take these and distill it into a final, consolidated summary of the Materials and Method section of this scientific paper in Markdown file format. Does the section provide sufficient details on how to replicate the study? Does it, for example, include descriptions on how the data was collected, which software and/or databases were used, etc? \"\n",
    "result = runMapReduce(map_template.format(question=map_q, docs=\"{docs}\"), reduce_template.format(description=reduce_qDescription, question = reduce_q, doc_summaries=\"{doc_summaries}\"), split_docs, llm=llm, model=MODEL)\n",
    "#print(result)\n",
    "summaryDoc += result\n",
    "\n",
    "map_q = \"Could you provide a summary of the Conclusion section? You might need to deduce the section contains the conclusion if the header is absent of labeled differently. If the document contains different sections of the paper you can answer with an empty string.\"\n",
    "reduce_qDescription = \"The following is set of summaries of the conclusion section of a scientific paper:\"\n",
    "reduce_q = \"Take these and distill it into a final, consolidated summary of the conclusion of this scientific paper in Markdown file format. Is the conclusion section well written? Do the conclusions provided match the results? Are you missing any conclusions? Provide a bullet list of the main conclusions in markdown format.\"\n",
    "result = runMapReduce(map_template.format(question=map_q, docs=\"{docs}\"), reduce_template.format(description=reduce_qDescription, question = reduce_q, doc_summaries=\"{doc_summaries}\"), split_docs, llm=llm, model=MODEL)\n",
    "#print(result)\n",
    "summaryDoc += result\n",
    "\n",
    "map_q = \"Could you provide a summary of the Discussion section (or that part in the paper which contains a discussion)? You might need to deduce the section contains the discussion if the header is absent of labeled differently. If the document contains different sections of the paper you can answer with an empty string.\"\n",
    "reduce_qDescription = \"The following is set of summaries of the conclusion section of a scientific paper:\"\n",
    "reduce_q = \"Take these and distill it into a final, consolidated summary of the discussion section of this scientific paper in Markdown file format. How well written is the discussion? Does it indeed contain sufficient topics? Are there any strange, out of context or maybe even wild statements? Does the discussion section align with the rest of the paper?\"\n",
    "result = runMapReduce(map_template.format(question=map_q, docs=\"{docs}\"), reduce_template.format(description=reduce_qDescription, question = reduce_q, doc_summaries=\"{doc_summaries}\"), split_docs, llm=llm, model=MODEL)\n",
    "#print(result)\n",
    "summaryDoc += result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summaryDoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide feedback on the paper\n",
    "\n",
    "Providing feedback is a very important part of being a scientific researcher and/or supervisor. We can ask the LLM to generate a list of tips and tops which you can use as input for the discussion with your peer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_q = \"You are a highly skilled researcher and writer. Could you reflect on this part of paper and provide feedback in the form of 'tips and tops'\"\n",
    "reduce_qDescription = \"The following is set of tips (things that can be improved)  and tops (things that are good about the text) as feedback provided by a highly skilled researcher and writer:\"\n",
    "reduce_q = \"Take these and distill it into a final, consolidated summary of tips (things that can be improved) and tops (things that are good about the text) in Markdown file format.\"\n",
    "result = runMapReduce(map_template.format(question=map_q, docs=\"{docs}\"), reduce_template.format(description=reduce_qDescription, question = reduce_q, doc_summaries=\"{doc_summaries}\"), split_docs, llm=llm, model=MODEL)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further feedback and research topics\n",
    "\n",
    "Now let's ask the LLM to dive a little deeper into the science of the paper. First, we ask it to verify the general outline of the paper. After that, we will ask the LLM to generate a list of similar research topics and finally we would like to have some ideas on follow-up research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversation chain\n",
    "\n",
    "Very similar to the direct call to the OpenAI Assistant API. If you like to use a single module, in stead of two, you can use langchain as your primary toolkit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_buf = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferMemory()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General outline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = conversation_buf(\"You are provided with summaries of the sections of a scientific paper. Does this paper follow the general outline of a scientific paper? The summaries in Markdown format: \" + summaryDoc)\n",
    "print(\"\\n\\n# Does the paper follow the general outline of a scientific paper?  \\n\\n\")\n",
    "print(answer['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similar research topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = conversation_buf.run(\"Could you provide a list of similar research topics and/or questions which are directly related to the topics described in this paper?\")\n",
    "print(\"\\n\\n# Similar research topics and/or questions related to this paper \\n\\n\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Follow-up research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = conversation_buf.run(\"Could you suggest follow-up research question based on the findings and discussion presented in this paper?\")\n",
    "print(\"\\n\\n# Suggestions for follow-up research \\n\\n\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean-up\n",
    "\n",
    "Normally, you would create the vector store and assistant only once. After creation, you can use the assistant.id for future reference. For now, let's clean everything up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[VectorStore(id='vs_kftdiCgZIKmvbUn5RPMOj7jk', created_at=1716846470, file_counts=FileCounts(cancelled=0, completed=0, failed=0, in_progress=0, total=0), last_active_at=1716846554, metadata={}, name='Research papers', object='vector_store', status='completed', usage_bytes=0, expires_after=None, expires_at=None), VectorStore(id='vs_s0iNOxcAT4VhquKAzhwDaxgU', created_at=1716846237, file_counts=FileCounts(cancelled=0, completed=0, failed=0, in_progress=0, total=0), last_active_at=1716846320, metadata={}, name='Bejo recipes', object='vector_store', status='completed', usage_bytes=0, expires_after=None, expires_at=None), VectorStore(id='vs_xeTI6iokxD3qyKkLD3aWQFFb', created_at=1716815561, file_counts=FileCounts(cancelled=0, completed=0, failed=0, in_progress=0, total=0), last_active_at=1716815620, metadata={}, name='Bejo recipes', object='vector_store', status='completed', usage_bytes=0, expires_after=None, expires_at=None), VectorStore(id='vs_kNYYucxUxtIQQUwhoTjgQ1wk', created_at=1716815517, file_counts=FileCounts(cancelled=0, completed=0, failed=0, in_progress=0, total=0), last_active_at=1716815517, metadata={}, name='Bejo recipes', object='vector_store', status='completed', usage_bytes=0, expires_after=None, expires_at=None), VectorStore(id='vs_Wei0d7d9SH4HU0edoEUikYys', created_at=1716815513, file_counts=FileCounts(cancelled=0, completed=0, failed=0, in_progress=0, total=0), last_active_at=1716815515, metadata={}, name='Bejo recipes', object='vector_store', status='completed', usage_bytes=0, expires_after=None, expires_at=None), VectorStore(id='vs_FjiqKNlBHHsTBVuv2ORji8GD', created_at=1716815480, file_counts=FileCounts(cancelled=0, completed=0, failed=0, in_progress=0, total=0), last_active_at=1716815480, metadata={}, name='Bejo recipes', object='vector_store', status='completed', usage_bytes=0, expires_after=None, expires_at=None), VectorStore(id='vs_1Bfi9DsZEAQuZGqJoWJc0SjP', created_at=1716815445, file_counts=FileCounts(cancelled=0, completed=0, failed=0, in_progress=0, total=0), last_active_at=1716815446, metadata={}, name='Bejo recipes', object='vector_store', status='completed', usage_bytes=0, expires_after=None, expires_at=None), VectorStore(id='vs_aTaaBsp36igwGx23NjYWUpmL', created_at=1716815014, file_counts=FileCounts(cancelled=0, completed=0, failed=0, in_progress=0, total=0), last_active_at=1716815046, metadata={}, name='Bejo recipes', object='vector_store', status='completed', usage_bytes=0, expires_after=None, expires_at=None), VectorStore(id='vs_2b0gajggNsXZYfw1VZqAAm7B', created_at=1716814677, file_counts=FileCounts(cancelled=0, completed=0, failed=0, in_progress=0, total=0), last_active_at=1716814700, metadata={}, name='Bejo recipes', object='vector_store', status='completed', usage_bytes=0, expires_after=None, expires_at=None), VectorStore(id='vs_pe9lPUFLRVcZEIuIT85I4nfV', created_at=1716814655, file_counts=FileCounts(cancelled=0, completed=0, failed=0, in_progress=0, total=0), last_active_at=1716814665, metadata={}, name='Bejo recipes', object='vector_store', status='completed', usage_bytes=0, expires_after=None, expires_at=None), VectorStore(id='vs_XiZGKgjIGZTvHboXb1XWg1W5', created_at=1716814603, file_counts=FileCounts(cancelled=0, completed=0, failed=0, in_progress=0, total=0), last_active_at=1716814757, metadata={}, name='Bejo recipes', object='vector_store', status='completed', usage_bytes=0, expires_after=None, expires_at=None), VectorStore(id='vs_5xe7LdRh6iOU7HJC5i8mYvY6', created_at=1716814576, file_counts=FileCounts(cancelled=0, completed=0, failed=0, in_progress=0, total=0), last_active_at=1716814576, metadata={}, name='Bejo recipes', object='vector_store', status='completed', usage_bytes=0, expires_after=None, expires_at=None), VectorStore(id='vs_X80qrui5RiEosBz8vA8ySZWB', created_at=1716814518, file_counts=FileCounts(cancelled=0, completed=0, failed=0, in_progress=0, total=0), last_active_at=1716815258, metadata={}, name='Research papers', object='vector_store', status='completed', usage_bytes=0, expires_after=None, expires_at=None), VectorStore(id='vs_2gcKqGymUkFBCJvIsXNhFsqg', created_at=1716814476, file_counts=FileCounts(cancelled=0, completed=0, failed=0, in_progress=0, total=0), last_active_at=1716814481, metadata={}, name='Research papers', object='vector_store', status='completed', usage_bytes=0, expires_after=None, expires_at=None), VectorStore(id='vs_ra9Qvdf06aQCxsuHkcI8AAgP', created_at=1716814463, file_counts=FileCounts(cancelled=0, completed=0, failed=0, in_progress=0, total=0), last_active_at=1716814641, metadata={}, name='Bejo recipes', object='vector_store', status='completed', usage_bytes=0, expires_after=None, expires_at=None), VectorStore(id='vs_ySyhgsQXyMljFOgjyG7biMOw', created_at=1716814324, file_counts=FileCounts(cancelled=0, completed=0, failed=0, in_progress=0, total=0), last_active_at=1716814985, metadata={}, name='Bejo recipes', object='vector_store', status='completed', usage_bytes=0, expires_after=None, expires_at=None), VectorStore(id='vs_t4JMMUeVE6yUUFlkTSXZT4R0', created_at=1716814270, file_counts=FileCounts(cancelled=0, completed=0, failed=0, in_progress=0, total=0), last_active_at=1716814275, metadata={}, name='Bejo recipes', object='vector_store', status='completed', usage_bytes=0, expires_after=None, expires_at=None), VectorStore(id='vs_jv58agSFXUokY9FulG7vUjfc', created_at=1716814094, file_counts=FileCounts(cancelled=0, completed=0, failed=0, in_progress=0, total=0), last_active_at=1716814099, metadata={}, name='Bejo recipes', object='vector_store', status='completed', usage_bytes=0, expires_after=None, expires_at=None), VectorStore(id='vs_IKkYxROzCFHc8FPdS0XU8iLs', created_at=1716813744, file_counts=FileCounts(cancelled=0, completed=0, failed=0, in_progress=0, total=0), last_active_at=1716815360, metadata={}, name='Bejo recipes', object='vector_store', status='completed', usage_bytes=0, expires_after=None, expires_at=None), VectorStore(id='vs_hvhktPpW3zbo6tBv1xs5fV9d', created_at=1716813623, file_counts=FileCounts(cancelled=0, completed=0, failed=0, in_progress=0, total=0), last_active_at=1716813625, metadata={}, name='Bejo recipes', object='vector_store', status='completed', usage_bytes=0, expires_after=None, expires_at=None)]\n",
      "Deleted file: vs_kftdiCgZIKmvbUn5RPMOj7jk\n",
      "Deleted file: vs_s0iNOxcAT4VhquKAzhwDaxgU\n",
      "Deleted file: vs_xeTI6iokxD3qyKkLD3aWQFFb\n",
      "Deleted file: vs_kNYYucxUxtIQQUwhoTjgQ1wk\n",
      "Deleted file: vs_Wei0d7d9SH4HU0edoEUikYys\n",
      "Deleted file: vs_FjiqKNlBHHsTBVuv2ORji8GD\n",
      "Deleted file: vs_1Bfi9DsZEAQuZGqJoWJc0SjP\n",
      "Deleted file: vs_aTaaBsp36igwGx23NjYWUpmL\n",
      "Deleted file: vs_2b0gajggNsXZYfw1VZqAAm7B\n",
      "Deleted file: vs_pe9lPUFLRVcZEIuIT85I4nfV\n",
      "Deleted file: vs_XiZGKgjIGZTvHboXb1XWg1W5\n",
      "Deleted file: vs_5xe7LdRh6iOU7HJC5i8mYvY6\n",
      "Deleted file: vs_X80qrui5RiEosBz8vA8ySZWB\n",
      "Deleted file: vs_2gcKqGymUkFBCJvIsXNhFsqg\n",
      "Deleted file: vs_ra9Qvdf06aQCxsuHkcI8AAgP\n",
      "Deleted file: vs_ySyhgsQXyMljFOgjyG7biMOw\n",
      "Deleted file: vs_t4JMMUeVE6yUUFlkTSXZT4R0\n",
      "Deleted file: vs_jv58agSFXUokY9FulG7vUjfc\n",
      "Deleted file: vs_IKkYxROzCFHc8FPdS0XU8iLs\n",
      "Deleted file: vs_hvhktPpW3zbo6tBv1xs5fV9d\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "# Function to list and delete files from OpenAI\n",
    "def delete_openai_files(client):\n",
    "    files = client.files.list()\n",
    "    print(files.data)\n",
    "    for file in files.data:\n",
    "        client.files.delete(file.id)\n",
    "        print(f\"Deleted file: {file.id}\")\n",
    "\n",
    "def delete_openai_vector_stores(client):\n",
    "    vectors = client.beta.vector_stores.list()\n",
    "    print(vectors.data)\n",
    "    for vector in vectors.data:\n",
    "        client.beta.vector_stores.delete(vector.id)\n",
    "        print(f\"Deleted vector: {vector.id}\")\n",
    "\n",
    "\n",
    "# Execute the deletion functions\n",
    "delete_openai_files(client)\n",
    "delete_openai_vector_stores(client)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
